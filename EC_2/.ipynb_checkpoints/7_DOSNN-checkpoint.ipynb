{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DOS 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import neccessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import numpy as np\n",
    "\n",
    "# tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "\n",
    "# I/O \n",
    "from numpy import loadtxt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Setting constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regime: 1000 1000 5\n"
     ]
    }
   ],
   "source": [
    "TEST = False\n",
    "\n",
    "# in this doc\n",
    "if TEST:\n",
    "    BATCH_SIZE = 7\n",
    "    EPOCHS = 10\n",
    "    PATH='test_data/'\n",
    "    FILE_NAME = 't'\n",
    "else:\n",
    "    BATCH_SIZE = 25\n",
    "    EPOCHS = 10\n",
    "    PATH='data/'\n",
    "    FILE_NAME = 'dos7'\n",
    "\n",
    "# other relavant parameters\n",
    "with open(PATH+FILE_NAME+'_params.txt') as f:\n",
    "    f.readline()\n",
    "    BOXLENGTH, DATA_SIZE, NEV = list(map(int, f.readline().split(','))) \n",
    "NAMES = ['dos', 'evs', 'W-evs', 'W', 'W-1', 'W-2', 'DW2', 'W-1DW2', 'W-2DW2', 'V-W', 'WxV-W', 'W2xV-W']\n",
    "\n",
    "print(\"Regime:\", BOXLENGTH, DATA_SIZE, NEV)\n",
    "#EPSILON = np.finfo(np.float32).tiny"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Loading training/testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(names=NAMES, path=PATH, file_name=FILE_NAME):        \n",
    "    train, test= [], []\n",
    "    for i in range(len(names)):\n",
    "        train.append(loadtxt(path + file_name + '_train_' + names[i] + '.cvs', delimiter=',').astype(np.float32))\n",
    "        test.append(loadtxt(path + file_name + '_test_' + names[i] + '.cvs', delimiter=',').astype(np.float32))\n",
    "    \n",
    "    return train, test\n",
    "\n",
    "# form data for training/testing\n",
    "# NOTE: cannot set data_size=DATA_SIZE as DATA_SIZE = 0 before loading\n",
    "def form_data(train, test, data_size=DATA_SIZE, batch_size=BATCH_SIZE): \n",
    "    for i in range(len(train)-2):\n",
    "        train[i+2] = train[i+2][..., tf.newaxis]\n",
    "        test[i+2] = test[i+2][..., tf.newaxis]\n",
    "    \n",
    "    dos_tr, evs_tr, W_tr = train[0], train[1], train[2]\n",
    "    aux_tr = tf.concat(train[3:], axis=2)\n",
    "    \n",
    "    dos_te, evs_te, W_te = test[0], test[1], test[2]\n",
    "    aux_te = tf.concat(test[3:], axis=2)            \n",
    "    \n",
    "    train_ds = tf.data.Dataset.from_tensor_slices( \\\n",
    "            (aux_tr, W_tr, evs_tr, dos_tr)).shuffle(data_size).batch(batch_size)\n",
    "    \n",
    "    test_ds = tf.data.Dataset.from_tensor_slices( \\\n",
    "            (aux_te, W_te, evs_te, dos_te)).batch(batch_size)\n",
    "    \n",
    "    return train_ds, test_ds\n",
    "\n",
    "def form_test_data(data, data_size=DATA_SIZE): \n",
    "    for i in range(len(data)-2):\n",
    "        data[i+2] = data[i+2][..., tf.newaxis]\n",
    "        \n",
    "    dos, evs, W_evs = data[0], data[1], data[2]\n",
    "    aux = tf.concat(data[3:], axis=2)\n",
    "    \n",
    "    return aux, W_evs, evs, dos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST:\n",
    "    train_test_data, test_test_data = load()\n",
    "    print('BOXLENGTH, DATA_SIZE, NEV:')\n",
    "    print(BOXLENGTH, DATA_SIZE, NEV)\n",
    "    train_ds, test_ds = form_data(train_test_data, test_test_data) \n",
    "    print(train_ds)\n",
    "    print(np.min(train_test_data[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Box Counting Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decider(Model):\n",
    "    def __init__(self, out_channel_size, kernel_size, strides=1):\n",
    "        super(Decider, self).__init__(name='')\n",
    "        \n",
    "        self.FD_conv = layers.Conv1D(out_channel_size, \\\n",
    "                                     kernel_size=kernel_size, \\\n",
    "                                     strides=strides, \\\n",
    "                                     padding='same')\n",
    "        self.FD_pert_activation = tf.keras.activations.tanh #layers.LeakyReLU()\n",
    "        self.FD_lead_activation = layers.LeakyReLU(alpha=0.01) # tf.keras.activations.sigmoid  #\n",
    "        \n",
    "        self.aux_conv = layers.Conv1D(out_channel_size, \\\n",
    "                                      kernel_size=kernel_size, \\\n",
    "                                      strides=strides, \\\n",
    "                                      padding='same')\n",
    "        self.aux_activation = tf.keras.activations.tanh #layers.LeakyReLU()\n",
    "    \n",
    "    def call(self, aux, W_evs):\n",
    "        aux = tf.concat([W_evs, aux], axis=2)\n",
    "        aux = self.aux_conv(aux)\n",
    "        aux = self.aux_activation(aux)\n",
    "        \n",
    "        W_evs = self.FD_conv(W_evs)\n",
    "        W_evs_pert = self.FD_pert_activation(W_evs)\n",
    "        W_evs_lead = self.FD_lead_activation(W_evs)\n",
    "        \n",
    "        return W_evs_lead + aux * W_evs_pert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST:\n",
    "    test_decider = Decider(3, 5)\n",
    "    train_test_data2, _ = load()\n",
    "    aux, W_evs, evs, dos = form_test_data(train_test_data2)\n",
    "    print(\"Aux input shape:\", aux.shape)\n",
    "    print(\"Decider output shape:\",test_decider(aux, W_evs).shape)\n",
    "    test_decider.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoxCounter(Model):\n",
    "    def __init__(self, decider_params, conv_params, dense_params):\n",
    "        super(BoxCounter, self).__init__(name='')\n",
    "        \n",
    "        self.deciders = []\n",
    "        self.convs = []\n",
    "        self.denses = []\n",
    "        \n",
    "        for params in decider_params:\n",
    "            self.deciders.append(Decider(*params))\n",
    "        \n",
    "        for filters, kernel_size, strides, padding in conv_params:\n",
    "            self.convs.append(layers.Conv1D(filters, kernel_size, strides=strides, padding=padding))\n",
    "            self.convs.append(layers.BatchNormalization())\n",
    "            self.convs.append(layers.LeakyReLU(alpha=0.01))\n",
    "        \n",
    "        for hid_size in dense_params:\n",
    "            self.denses.append(layers.Dense(hid_size))\n",
    "            self.denses.append(layers.BatchNormalization())\n",
    "            self.denses.append(layers.LeakyReLU(alpha=0.01))\n",
    "    \n",
    "    def call(self, aux, W_evs):\n",
    "        for decider in self.deciders:\n",
    "            W_evs = decider(aux, W_evs)\n",
    "        \n",
    "        for layer in self.convs:\n",
    "            W_evs = layer(W_evs)\n",
    "        \n",
    "        # squeeze\n",
    "        if W_evs.shape[0] != 1:\n",
    "            W_evs = tf.squeeze(W_evs)\n",
    "        else:\n",
    "            W_evs = W_evs[0]\n",
    "        \n",
    "        for layer in self.denses:\n",
    "            W_evs = layer(W_evs)\n",
    "        \n",
    "        return W_evs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1 forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if TEST:\n",
    "    test_box_counter = BoxCounter([[10, 3], [20, 3]], \n",
    "                                  [[30, 15, 10, 'same'], [50, 10, 10, 'same']],\n",
    "                                  [40, 20, 1]\n",
    "                                 )\n",
    "    print(test_box_counter(aux, W_evs).shape)\n",
    "    test_box_counter.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_ds=None, test_ds=None, epochs=EPOCHS):\n",
    "    loss_object = tf.keras.losses.MeanSquaredError() #MeanAbsoluteError() #   #MeanAbsolutePercentageError()\n",
    "    optimizer = tf.keras.optimizers.Adam()\n",
    "    \n",
    "    train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "    test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "    \n",
    "    @tf.function\n",
    "    def train_step(aux, W_evs, target):\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = model(aux, W_evs)\n",
    "            loss = loss_object(target, predictions)\n",
    "            gradients = tape.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "            train_loss(loss)\n",
    "            \n",
    "            \n",
    "    @tf.function\n",
    "    def test_step(aux, W_evs, target):\n",
    "        predictions = model(aux, W_evs)\n",
    "        #print(predictions.shape, target.shape)\n",
    "        t_loss = loss_object(target, predictions)\n",
    "\n",
    "        test_loss(t_loss)\n",
    "    \n",
    "    \n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Reset the metrics at the start of the next epoch\n",
    "        train_loss.reset_states()\n",
    "        test_loss.reset_states()\n",
    "\n",
    "        for aux, W_evs, evs, target  in train_ds:\n",
    "            train_step(aux, W_evs, target)\n",
    "\n",
    "        for aux, W_evs, evs, target in test_ds:\n",
    "            test_step(aux, W_evs, target)\n",
    "\n",
    "        template = 'Epoch {}, Training Loss: {}, Test Loss: {}'\n",
    "        print(template.format(epoch+1,\n",
    "                            train_loss.result(),\n",
    "                            test_loss.result()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(train_ds, test_ds, sample, epochs=EPOCHS, nev=NEV, boxlength=BOXLENGTH):\n",
    "    \n",
    "    # defining models\n",
    "    model = BoxCounter([[5, 5], [10, 5]], \n",
    "                                  [[15, 20, 20, 'same'], [30, 50, 50, 'same']],\n",
    "                                  [40, 20, 1]\n",
    "                                 )\n",
    "    \n",
    "    # training\n",
    "    print(\"-------------------------------------------\")\n",
    "    print(\"| Starting training for 7_DOSNN\")\n",
    "    print(\"-------------------------------------------\")\n",
    "\n",
    "    train(model, train_ds=train_ds, test_ds=test_ds, epochs=epochs)\n",
    "    print(\"\")\n",
    "    print(model.summary())\n",
    "    print(\"\")\n",
    "    print(\"Training finished\\n\")\n",
    "    \n",
    "    \n",
    "    # displaying some numerical values\n",
    "    print(\"-------------------------------------------\")\n",
    "    print(\"| Displaying numerical values for comparison\")\n",
    "    print(\"-------------------------------------------\")\n",
    "    print(\"True DOS:\")\n",
    "    print(sample[-1][:nev])\n",
    "    print(sample[-2][:nev])\n",
    "    #print(sample)\n",
    "    \n",
    "    \n",
    "    pred = model(tf.squeeze(sample[0][:nev]), tf.squeeze(sample[1][:nev])[...,tf.newaxis])\n",
    "    print(\"\")\n",
    "    print(\"Results from 7_DOSNN\")\n",
    "    print(pred)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_data, test_data = load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, test_ds = form_data(train_data, test_data)\n",
    "sample = form_test_data(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "| Starting training for 7_DOSNN\n",
      "-------------------------------------------\n",
      "Epoch 1, Training Loss: 1.8542366027832031, Test Loss: 0.7324868440628052\n",
      "Epoch 2, Training Loss: 0.7258827090263367, Test Loss: 0.6042999029159546\n",
      "Epoch 3, Training Loss: 0.586311936378479, Test Loss: 0.5215809941291809\n",
      "Epoch 4, Training Loss: 0.5007670521736145, Test Loss: 0.4641295075416565\n",
      "Epoch 5, Training Loss: 0.42877665162086487, Test Loss: 0.4542863368988037\n",
      "Epoch 6, Training Loss: 0.38913825154304504, Test Loss: 0.3919368088245392\n",
      "Epoch 7, Training Loss: 0.3526938557624817, Test Loss: 0.37232595682144165\n",
      "Epoch 8, Training Loss: 0.3113054037094116, Test Loss: 0.36863356828689575\n",
      "Epoch 9, Training Loss: 0.30391979217529297, Test Loss: 0.35524800419807434\n",
      "Epoch 10, Training Loss: 0.28933146595954895, Test Loss: 0.35195639729499817\n",
      "\n",
      "Model: \"box_counter_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "decider_4 (Decider)          multiple                  285       \n",
      "_________________________________________________________________\n",
      "decider_5 (Decider)          multiple                  970       \n",
      "_________________________________________________________________\n",
      "conv1d_16 (Conv1D)           multiple                  3015      \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc multiple                  60        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_16 (LeakyReLU)   multiple                  0         \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           multiple                  22530     \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc multiple                  120       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_17 (LeakyReLU)   multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              multiple                  1240      \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc multiple                  160       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_18 (LeakyReLU)   multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              multiple                  820       \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc multiple                  80        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_19 (LeakyReLU)   multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              multiple                  21        \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc multiple                  4         \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_20 (LeakyReLU)   multiple                  0         \n",
      "=================================================================\n",
      "Total params: 29,305\n",
      "Trainable params: 29,093\n",
      "Non-trainable params: 212\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "Training finished\n",
      "\n",
      "-------------------------------------------\n",
      "| Displaying numerical values for comparison\n",
      "-------------------------------------------\n",
      "True DOS:\n",
      "[1. 2. 3. 4. 5.]\n",
      "[0.20662192 0.2548387  0.2743091  0.282328   0.2854963 ]\n",
      "\n",
      "Results from 7_DOSNN\n",
      "tf.Tensor(\n",
      "[[0.9879962]\n",
      " [1.7180378]\n",
      " [3.3804476]\n",
      " [4.323492 ]\n",
      " [4.5255075]], shape=(5, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "compare_models(train_ds, test_ds, sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
